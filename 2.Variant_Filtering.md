---
title: "2.Variant_Filtering"
author: "Enrico"
date: "30 April 2019"
output: html_document
---

Next step in the pipeline is to filter unwanted variants from the final VCF file.

This are the criteria I will use to filter variants:

(1) Repetitive/Low mappability regions
(2) Indels + Non-biallelic sites
(3) Lynx genus wide exclusive substitutions from Felis catus
(4,5) Hard quality filters, as GATK standard practices
(6) Under-represented, excessively missing variants
(7) Over/Under covered regions (<5% and >95% of depth distribution)
(8) Genes?

I will try these filters individually in an interactive node of FT2 using a single chromosome VCF to see if any errors appear (see below for trial commands/scripts - keep in mind that the paths to files might not be well defined and can be inconsistent). After these trials I will write the scripts to apply to the final, whole-genome VCF.

Filters 1 to 4 can be applied to the whole dataset. Filters 5 and 6 need to be calculated at the level of the individual/species. For this reason I will write a first "General-Filters" script, that applies filters 1 to 4.

## (1) Repetitive/Low mappability regions

In order to filter repetitive and low mappability regions I needed to extract the coordinates of those regions in a BED format. The reference genome has a version (marked as rm in the file name = Felis_catus.Felis_catus_9.0.dna_rm.toplevel.fa) with these regions masked with Ns:

a TAGAGAGAGAGATT segment would be TAGNNNNNNNNATT

More information on how these regions were masked can be found in the README file of the reference genome folder available in the ensembl database.

With a python script I found online (at https://www.danielecook.com/generate-a-bedfile-of-masked-ranges-a-fasta-file/) I was able to create the BED file with the masked regions coordinates. You can find the python script in 2.Variant_Filtering-executables/generate_masked_ranges.py

```
# This was done in the $LUSTRE/test/CatGenome_Masked_BEDs/ folder in ft2 where the script
# and the masked reference genome are

./generate_masked_ranges.py Felis_catus.Felis_catus_9.0.dna_rm.toplevel.fa > Masked_Regions.bed
```

Let's try to filter the A1 chromosome for these coordinates.

```
# Initial number of variants:
zcat $LUSTRE/test/CatRef_vcfs/A1_cat_ref.vcf.gz | grep -v "#" | wc -l # 7417329

# Apply the filter with BedTools subtract
zcat $LUSTRE/test/CatRef_vcfs/A1_cat_ref.vcf.gz | bedtools subtract -a - -b $LUSTRE/test/CatGenome_Masked_BEDs/Masked_Regions.bed -header | uniq > $LUSTRE/test/FilterTrials/A1_cat_ref.vcf

# Number of variants after filtering:
grep -v "#" $LUSTRE/test/FilterTrials/A1_cat_ref.vcf | wc -l # 3801893

# Number of variants filtered:
# 7417329 - 3801893 = 3615436
```

## (2) Indels + Non-biallelic sites

I want to filter out INDELs and Non-Biallelic positions, because they represent a kind of data which is much harder to analyze.

I will use the GATK tool SelectVariants which has a two flags specifically for filtering these types of variants.

As before, let's try using this command with our A1 VCF file.

```
# Apply the filter with GATK SelectVariants
gatk SelectVariants \
  -select-type SNP \
  --restrict-alleles-to BIALLELIC \
  -R $LUSTRE/test/Felis_catus_Ref/Felis_catus.Felis_catus_9.0.dna.toplevel.fa \
  -V $LUSTRE/test/FilterTrials/A1_cat_ref.vcf \
  -O $LUSTRE/test/FilterTrials/A1_Biall_SNPs_cat_ref.vcf

# Number of variants after filtering:
cat $LUSTRE/test/FilterTrials/A1_Biall_SNPs_cat_ref.vcf | grep -v "#" | wc -l # 3191752

# Number of variants filtered:
# 3801893 - 3191752 = 610141
```

## (3) Lynx genus wide exclusive substitutions from Felis catus

As positions which are fixed in the whole Lynx genus are not informative to reconstruct evolutionary history within the genus, I will filter out of the VCF every position with allele frequency equal to 1.00 (same variant in all individuals).

This will be done with BCFtools view command. You can define regular expression which will determine which variants will be filtered in the VCF. In our case we want to eliminate every SNP with AF=1.00

```
# Apply the filter with BCFtools view
bcftools view -e 'INFO/AF=1.00' $LUSTRE/test/FilterTrials/A1_Biall_SNPs_cat_ref.vcf \
> $LUSTRE/test/FilterTrials/A1_Biall_SNPs_nonfix_cat_ref.vcf

# Number of variants after filtering:
cat $LUSTRE/test/FilterTrials/A1_Biall_SNPs_nonfix_cat_ref.vcf | grep -v "#" | wc -l # 1638053

# Number of variants filtered:
# 3191752 - 1638053 = 1553699
```

## (4,5) Hard quality filters, as GATK standard practices

Before applying GATK's suggested hard filters, I want to see the distribution of the values to filter in my dataset, to compare it with the distribution in the data used by GATK to calculate the values for the hard filters. This will allow me to check if the filters values fit my data as well.

The values of QUAL, QD, FS, MQ, MQRankSum, ReadPosRankSum will be evaluated (see GATK variant annotations at https://software.broadinstitute.org/gatk/documentation/tooldocs/current/ for more information on each).

I'll create a table with the values of interest from the filtered VCF, and then use R to analyze it.

I will use only chromosome A1 filtered VCF to try (A1_Biall_SNPs_nonfix_cat_ref.vcf from step 3).

```
# Number of variants:
grep -v "#" A1_Biall_SNPs_nonfix_cat_ref.vcf | wc -l # 1638053

# Number of variants with MQ annotated:
grep -v "#" A1_Biall_SNPs_nonfix_cat_ref.vcf | grep -o -E 'MQ=[[:digit:]]{1,3}\.?[[:digit:]]{0,3}' | wc -l # 1638021
# Missing MQ for positions with a 1/1 genotype called although there is 0 coverage for it!
# Extract column:
grep -v "#" A1_Biall_SNPs_nonfix_cat_ref.vcf | grep -o -E 'MQ=[[:digit:]]{1,3}\.?[[:digit:]]{0,3}' | cut -d '=' -f2 > MQ.table

# Number of variants with QD annotated:
grep -v "#" A1_Biall_SNPs_nonfix_cat_ref.vcf | grep -o -E 'QD=[[:digit:]]{1,3}\.?[[:digit:]]{0,3}' | wc -l # 1637966
# Missing QD for positions with a 1/1 genotype called although there is 0 coverage for it!
# Extract QD column:
grep -v "#" A1_Biall_SNPs_nonfix_cat_ref.vcf | grep -o -E 'QD=[[:digit:]]{1,3}\.?[[:digit:]]{0,3}' | cut -d '=' -f2 > QD.table

# Number of variants with FS annotated:
grep -v "#" A1_Biall_SNPs_nonfix_cat_ref.vcf | grep -o -E 'FS=[[:digit:]]{1,3}\.?[[:digit:]]{0,3}' | wc -l # 1638053
# Extract FS column:
grep -v "#" A1_Biall_SNPs_nonfix_cat_ref.vcf | grep -o -E 'FS=[[:digit:]]{1,3}\.?[[:digit:]]{0,3}' | cut -d '=' -f2 > FS.table

# Number of variants with MQRankSum annotated:
grep -v "#" A1_Biall_SNPs_nonfix_cat_ref.vcf | grep -o -E 'MQRankSum=\-?[[:digit:]]{1,3}\.?[[:digit:]]{0,3}' | wc -l # 852139
# Extract MQRankSum column:
grep -v "#" A1_Biall_SNPs_nonfix_cat_ref.vcf | grep -o -E 'MQRankSum=\-?[[:digit:]]{1,3}\.?[[:digit:]]{0,3}' | cut -d '=' -f2 > MQRankSum.table

# Number of variants with ReadPosRankSum annotated:
grep -v "#" A1_Biall_SNPs_nonfix_cat_ref.vcf | grep -o -E 'ReadPosRankSum=\-?[[:digit:]]{1,3}\.?[[:digit:]]{0,3}' | wc -l # 847953
# Extract ReadPosRankSum column:
grep -v "#" A1_Biall_SNPs_nonfix_cat_ref.vcf | grep -o -E 'ReadPosRankSum=\-?[[:digit:]]{1,3}\.?[[:digit:]]{0,3}' | cut -d '=' -f2 > ReadPosRankSum.table
```
Seeing that MQRankSum and ReadPosRankSum are calculated only when the Reference and Alternative bases are both found in at least one individual (half of our variants in A1, as calculated above), I will filter in a way that it won't remove positions where MQRankSum and ReadPosRankSum are not calculated. That is: instead of asking to include only the positions where the value is above a certain threshold, I'll tell the program to exclude the positions where the value is below that threshold; this way if the value is not found, the position is kept. Remember this is for MQRankSum and ReadPosRankSum ONLY.

After having analyzed the distributions with R, I see that they go way above the suggested filter thresholds, and the number of loci remaining after filtering is >99% of the number before applying the filter. I might consider stricter filters, although these threshold were selected by GATK as a way to remove false positive without being too strict and risking to remove true positives. Still need to understand why some positions have a 1/1 genotype with 0 reads supporting the alternative allele, but these positions will be filtered out anyway. I'll see if after applying all of the filters the problems will be fixed. If not I might consider simply filtering any variant with "1/1:0,0:0".

I'll try to filter the A1 filtered in the previous steps.
```
# Filter all except for the RanksSums:
gatk SelectVariants \
  --selectExpressions "QUAL >= 30 && QD >= 2.0 && FS <= 60.0 && MQ >= 40.0" \
  -R $LUSTRE/test/Felis_catus_Ref/Felis_catus.Felis_catus_9.0.dna.toplevel.fa \
  -V $LUSTRE/test/FilterTrials/A1_Biall_SNPs_nonfix_cat_ref.vcf \
  -O $LUSTRE/test/FilterTrials/A1_Biall_SNPs_nonfix_hardfiltered_cat_ref.vcf

# Number of variants:
grep -v "#" A1_Biall_SNPs_nonfix_hardfiltered_cat_ref.vcf | wc -l # 1610638
# Number of variants filtered:
# 1638053 - 1610638 = 27415


# Filter RankSums with bcftools view:
bcftools view -e 'INFO/MQRankSum<-12.5 | INFO/ReadPosRankSum<-8.0' $LUSTRE/test/FilterTrials/A1_Biall_SNPs_nonfix_hardfiltered_cat_ref.vcf \
> $LUSTRE/test/FilterTrials/A1_Biall_SNPs_nonfix_hardfiltered_ranksums_cat_ref.vcf

# Number of variants:
grep -v "#" A1_Biall_SNPs_nonfix_hardfiltered_ranksums_cat_ref.vcf | wc -l # 1610633
# Number of variants filtered:
# 1610638 - 1610633 = 5
```
## (1,2,3,4,5) Applying General Filters

The script applying filters 1 through 5, which are independent of species and sequencing technology, and can therefore be applied to the whole dataset can be found at 2.Variant_Filtering-executables/General_Filter_1-2-3-4-5.sh

```
# Launch General_Filter_1-2-3-4-5.sh on CESGA FT2:
sbatch General_Filter_1-2-3-4-5.sh WholeGenome_cat_ref
```

## (6) Under-represented, excessively missing variants

It's important to filter out variants which are missing completely in one or more species. For this I will divide the VCF into 4 different per-species VCFs and generate 4 lists of variants with no genotype called in any individual (of that species).

I'll first generate a file with the list of sample names for each species.

Then I can use these files to give the names of the samples I want to extract from the VCF file to BCFtools. So I will use BCFtools view in a loop for each species to generate a per-species VCF.

With the per-species VCF, I can again use BCFtools again to filter out variants with missing genotypes for too many samples. Each species will have a different value of required number of samples with missing genotype (as the sample size is different for each species). The different requirements for different species will be demanded through an IF statement. By telling BCFtools to include only those variants (-i), the output can be used as a list of positions to exclude from the original VCF.

The script ( 2.Variant_Filtering-executables/Missingness_Filter_6.sh ) will be launched on CESGA's FT2 server queue system.

```
# Launch Missingness_Filter_6.sh on CESGA FT2:
sbatch Missingness_Filter_6.sh WholeGenome_cat_ref
```

Adding the final number of variants to the log file created by the Missingness_Filter_6.sh script

```
# Total number of variants in WholeGenome_cat_ref.filter6.vcf :
echo "Variants in filter6.vcf : 15620540" >> missingness.variants.log

# and the difference with WholeGenome_cat_ref.filter5.vcf :
echo "Total variants filtered : 592282" >> missingness.variants.log
```

## (6.5) Under-represented variants and Below minimum depth threshold genotypes

I wanted to explore the possibility of filtering out the genotypes called with the support of a very low number of reads (low depth). I found there is a command of GATK that will turn any genotype with DP (depth) below a certain threshold into "no-calls" (a ./. genotype).

A good way to apply this filter would be to do it before filtering out excessively missing variants, because adding no-calls would increase their number. A different version of step 6 should be considered. The drawback of this filtering method is that while removing genotypes, not all of the 'INFO' field annotations will be recalculated. In fact only AC (Allele Count), AN (Allele Number) and AF (Allele Frequency) will have their value recalculated after the removal of low depth genotypes.

Is it worth it?

## (7) Over/Under covered regions (<5% and >95% of depth distribution)

Before calculating coverage and depth distributions I want to first divide my VCF file by species and sequencing technology, as they represent key factors in determining which areas of the genome are more or less sequenced (species specific structural variation might cause the different sequencing machines to work differently; species specific duplications; etc.).

This is how the VCF will be divided:

Lynx lynx - one dasaset :
  group 1 - cr_0212, ki_0090, vl_0112, ya_0146 with MACROGEN : LLmacro

Lynx rufus - three dasasets :
  group 1 -  c_lr_fl_0005 from Murphy : LRMurphy
  group 2 - c_lr_nm_0006 from Janecka : LRJan
  group 3 - c_lr_zz_0001 with MACROGEN : LRmacro

Lynx canadiensis - two dasasets :
  group 1 - c_lc_zz_0001 with MACROGEN : LCmacro
  group 2 - c_lc_zz_0003 from Murphy : LCMurphy

Lynx pardinus - two dasasets :
  group 1 -  c_lp_do_0153, c_lp_do_0173, c_lp_do_0443, c_lp_sm_0138, c_lp_sm_0140, c_lp_sm_0185, c_lp_sm_0186, c_lp_sm_0298, c_lp_sm_0359, h_lp_do_0007 from Proyecto Genoma : LPpgenoma
  group 2 - c_lp_sm_0221 from CANDILES : LPcandiles

I'll create BAMlist files (a file with a list of BAMs) for each dasaset

```
# Create DATSET bamlists

# Lynx lynx - one dasaset :
# group 1 - cr_0212, ki_0090, vl_0112, ya_0146 with MACROGEN : LLmacro
ls $LUSTRE/test/CatRef_bams/c_ll_*_cat_ref_sorted_rg_rmdup_sorted_indelrealigner.bam > $LUSTRE/test/CatRef_bams/LLmacro.bamlist

# Lynx rufus - three dasasets :
# group 1 -  c_lr_fl_0005 from Murphy : LRMurphy
ls $LUSTRE/test/CatRef_bams/c_lr_fl_0005_cat_ref_sorted_rg_rmdup_sorted_indelrealigner.bam > $LUSTRE/test/CatRef_bams/LRMurphy.bamlist
# group 2 - c_lr_nm_0006 from Janecka : LRJan
ls $LUSTRE/test/CatRef_bams/c_lr_nm_0006_cat_ref_sorted_rg_rmdup_sorted_indelrealigner.bam > $LUSTRE/test/CatRef_bams/LRJan.bamlist
# group 3 - c_lr_zz_0001 with MACROGEN : LRmacro
ls $LUSTRE/test/CatRef_bams/c_lr_zz_0001_cat_ref_sorted_rg_rmdup_sorted_indelrealigner.bam > $LUSTRE/test/CatRef_bams/LRmacro.bamlist

# Lynx canadiensis - two dasasets :
# group 1 - c_lc_zz_0001 with MACROGEN : LCmacro
ls $LUSTRE/test/CatRef_bams/c_lc_zz_0001_cat_ref_sorted_rg_rmdup_sorted_indelrealigner.bam > $LUSTRE/test/CatRef_bams/LCmacro.bamlist
# group 2 - c_lc_zz_0003 from Murphy : LCMurphy
ls $LUSTRE/test/CatRef_bams/c_lc_zz_0003_cat_ref_sorted_rg_rmdup_sorted_indelrealigner.bam > $LUSTRE/test/CatRef_bams/LCMurphy.bamlist

# Lynx pardinus - two dasasets :
# group 1 -  c_lp_do_0153, c_lp_do_0173, c_lp_do_0443, c_lp_sm_0138, c_lp_sm_0140, c_lp_sm_0185, c_lp_sm_0186, c_lp_sm_0298, c_lp_sm_0359, h_lp_do_0007 from Proyecto Genoma : LPpgenoma
ls $LUSTRE/test/CatRef_bams/*_lp_*_cat_ref_sorted_rg_rmdup_sorted_indelrealigner.bam | grep -v "c_lp_sm_0221" > $LUSTRE/test/CatRef_bams/LPpgenoma.bamlist
# group 2 - c_lp_sm_0221 from CANDILES : LPcandiles
ls $LUSTRE/test/CatRef_bams/c_lp_*_cat_ref_sorted_rg_rmdup_sorted_indelrealigner.bam | grep "c_lp_sm_0221" > $LUSTRE/test/CatRef_bams/LPcandiles.bamlist
```

As the datasets are very big and we only need a representative subset of the data, I will subsample the BAM files. To do that I will generate a BED file with 200 random positions of 100000 bp length. The depth calculations will be done considering only this random subset.

To generate the random regions file I will use BEDtools random. I'll then remove the low-mappability and repetitive regions from the file using BEDtools subtract

```
# Create a Genome region file for Bedtools:
# A file with the list of chromosomes as col1 and their length as col2, tab separated
# Basically the first two columns of a FAI file:
cut -f1,2 $LUSTRE/test/Felis_catus_Ref/Felis_catus.Felis_catus_9.0.dna.toplevel.fa.fai > \
$LUSTRE/test/Felis_catus_Ref/Felis_catus.Felis_catus_9.0.dna.toplevel.genome

# Bedtools random to generate file of 200 random segments of 100000 bp
# Output a BED file:
bedtools random -l 100000 -n 200 -g $LUSTRE/test/Felis_catus_Ref/Felis_catus.Felis_catus_9.0.dna.toplevel.genome | \
sort > $LUSTRE/test/FilterTrials/Felis_catus.200x100kbp.genome.bed

# Using bedtools subtract I can remove low-mappability and repetitive regions:
bedtools subtract -a $LUSTRE/test/FilterTrials/Felis_catus.200x100kbp.genome.bed -b $LUSTRE/test/CatGenome_Masked_BEDs/Masked_Regions.bed > $LUSTRE/test/FilterTrials/Felis_catus.200x100kbp.masked.genome.bed
```

I will calculate depth at each position using samtools depth. Depth at all positions will be calculated (-a) within the regions randomly selected before (-b). This will be run in a loop for all bamlists, within a script (because from an interactive node on Cesga ft2 it might disconnect) : see 2.Variant_Filtering-executables/samtools_depth.sh

```
# Submit Samtools Depth calculator:
sbatch samtools_depth.sh
```

These tables of per-position depth values will be analyzed in R (see 2.Variant_Filtering-executables/depth_loop.R).

The output table generated by the R script, has information regarding mean depth and standard deviation, and also the calculated upper and lower limits (with different criteria - see the R script for more info) of the DP (depth) values for each dataset. The table will be used to recall these values when applying the depth filter (see 2.Variant_Filtering-executables/Depth_Filter_7.sh)

```
# Submit Depth filtering job on CESGA ft2:
sbatch Depth_Filter_7.sh WholeGenome_cat_ref
```
