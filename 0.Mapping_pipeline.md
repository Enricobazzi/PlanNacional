---
title: "0.Mapping_pipeline"
author: "Enrico"
date: "30 January 2019"
output: html_document
---

In order to calculate demographic models with machine learning methods (step carried by other group = Oscar Lao), high coverage sequencing data for at least 2 individuals per species will be mapped to latest version of the cat reference genome (version 9.0 -> downloaded from ensembl and copied to /home/GRUPOS/grupolince/reference_genomes/felis_catus_genome folder).

In this markdown I will go through the steps necessary for the mapping pipeline, to test each step with only one individual from the MACROGEN sequencing, LL112 (c_ll_vl_0112). The executable files generated afterwards (for mapping the rest of the samples), go through the same steps.

```

# BEFORE STARTING
# Reference Genome needs indexing
samtools faidx /home/GRUPOS/grupolince/reference_genomes/felis_catus_genome/Felis_catus.Felis_catus_9.0.dna.toplevel.fa

java -jar /opt/picard-tools/picard.jar CreateSequenceDictionary R= /home/GRUPOS/grupolince/reference_genomes/felis_catus_genome/Felis_catus.Felis_catus_9.0.dna.toplevel.fa O= /home/GRUPOS/grupolince/reference_genomes/felis_catus_genome/Felis_catus.Felis_catus_9.0.dna.toplevel.dict

bwa index /home/GRUPOS/grupolince/reference_genomes/felis_catus_genome/Felis_catus.Felis_catus_9.0.dna.toplevel.fa

```

Since the samples were already quality checked and trimmed for previous projects, these steps were skipped and I proceeded to map directly.

# Path Definition

Define the paths to different locations

```

MacroGenARRAY=($(ls /backup/grupolince/raw_data/MACROGEN/MACROGEN_trimmed/*.fastq.gz | rev | cut -d'/' -f 1 | rev | cut -d '_' -f1 | uniq)) # List of all Lynx lynx sample codes in MACROGEN project

REF=/home/GRUPOS/grupolince/reference_genomes/felis_catus_genome/Felis_catus.Felis_catus_9.0.dna.toplevel.fa # path to cat reference genome

THREADS=10 # No. of computer cores used by bwa and samtools. 20 = OK, >20 = ask people first!

OUT=/home/ebazzicalupo/CatRef_bams # path to output files

MacroGenPATH=/backup/grupolince/raw_data/MACROGEN/MACROGEN_trimmed/ # path to macrogen fastq files


# BARCODE MACROGEN:
declare -A BARCODEID=(["LC1"]="c_lc_zz_0001" ["LL112"]="c_ll_vl_0112" ["LL146"]="c_ll_ya_0146" ["LL212"]="c_ll_cr_0212" ["LL90"]="c_ll_ki_0090" ["LR1"]="c_lr_zz_0001")

```

# 	Mapping BWA-MEM

```

# Trial with only second (1) array element -> LL112

screen -S CatRef_Mapping.log
script CatRef_Mapping.log

for i in ${MacroGenARRAY[1]}
  do
    echo " - mapping ${i} -"
    bwa mem $REF $MacroGenPATH/${i}_R1_trimmed.fastq.gz $MacroGenPATH/${i}_R2_trimmed.fastq.gz -t $THREADS | samtools view -hbS -@ $THREADS - -o $OUT/${i}.cat_ref.bam
    echo " - sorting ${i} -"
    samtools sort -@ $THREADS $OUT/${i}.cat_ref.bam -o $OUT/${i}.cat_ref.sorted.bam && rm $OUT/${i}.cat_ref.bam
    echo " - done -"
done

```

## 	Add Read Groups and Change BAM names

Some of the merging part of this script was not actually used. As Maria was merging different sequencing projects at the same time she needed to include an "if statement" (see below) to see if merging was actually necessary. As I mapped different projects at different times, in the end I knew if merging was necessary or not for each, so I removed the "if statement" and included the merging step only when necessary.

```

# Trial with only second (1) array element -> LL112

# Read Groups

screen -S ReadGroups.log
script ReadGroups.log

for i in ${MacroGenARRAY[1]}
  do
    run=($(echo $i | cut -d"_" -f 1))  #Sacar el run de i
    echo $run
    java -jar /opt/picard-tools/picard.jar AddOrReplaceReadGroups I=$OUT/${i}.cat_ref.sorted.bam O=$OUT/${BARCODEID["${i}"]}_${i}.cat_ref.sorted.rg.bam RGID=${i} RGLB=${BARCODEID["${i}"]}_lib RGPL=Illumina RGPU=${run} RGSM=${BARCODEID["${i}"]} VALIDATION_STRINGENCY=SILENT && rm $OUT/${i}.cat_ref.sorted.bam
done

# Merge BAMs / Change BAM names

SAMPLESLIST=($(echo ${BARCODEID[@]} | tr ' ' '\n' | sort | uniq | grep c_ll_vl_0112))
for sample in "${SAMPLESLIST[@]}"
  do
    echo "${sample}"
    ls $OUT/"${sample}"_*.cat_ref.sorted.rg.bam  > $OUT/"${sample}".bam.list
    echo;echo
    echo `wc -l $OUT/"${sample}".bam.list`;
    lines=`wc -l $OUT/"${sample}".bam.list | cut -f 1 -d " " `
    if [ "$lines" -eq "1" ];
      then echo "ONE";
      cp `cat $OUT/"${sample}".bam.list`  $OUT/"${sample}"_sorted.bam;
    elif [ "$lines" -gt "1" ];
      then echo "more than ONE";
      samtools merge  -@ $THREADS -r $OUT/"${sample}".bam `cat $OUT/"${sample}".bam.list`;
      samtools sort  -@ $THREADS $OUT/"${sample}".bam -o $OUT/"${sample}"_sorted.bam && rm $OUT/"${sample}".bam;
    fi
    samtools flagstat $OUT/"${sample}"_sorted.bam >  $OUT/"${sample}".stats;
done

```

## 	Remove / Mark Duplicates

```

screen -S MDUP.log
script MDUP.log

ARRAY_MERGED_BAM_SAMPLE_NAME=($(ls $OUT/*_sorted.bam | rev | cut -d'/' -f1 | rev | cut -d'_' -f1-4 | uniq))
for i in ${ARRAY_MERGED_BAM_SAMPLE_NAME[@]}
  do
  echo "${i}"
  java -jar /opt/picard-tools/picard.jar MarkDuplicates METRICS_FILE=${i}_rmdup.txt I=$OUT/${i}_sorted.bam O=$OUT/${i}_sorted_rmdup.bam MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=800
  rm $OUT/${i}_sorted.bam
  samtools sort $OUT/${i}_sorted_rmdup.bam -@ 10 -o $OUT/${i}_sorted_rmdup_sorted.bam
  rm $OUT/${i}_sorted_rmdup.bam
  samtools index $OUT/${i}_sorted_rmdup_sorted.bam
  samtools flagstat $OUT/${i}_sorted_rmdup_sorted.bam > $OUT/${i}_sorted_rmdup_sorted.stats
done

```

##  Realign

```

screen -S realign.log
script realign.log

ARRAY_MERGED_BAM_SAMPLE_NAME=($(ls $OUT/*_sorted.bam | rev | cut -d'/' -f1 | rev | cut -d'_' -f1-4 | uniq))

## Not INCLUDING -fixMisencodedQuals flag because MACROGEN are Illumina 1.9 (quality code doesn't need fixing)

for i in ${ARRAY_MERGED_BAM_SAMPLE_NAME[@]}
  do
    echo "${i}"
    samtools index $OUT/${i}_sorted_rmdup_sorted.bam
    # RealignerTargetCreator
    java -jar /home/tmp/Software/GATK_3.4/GenomeAnalysisTK.jar -T RealignerTargetCreator -nt 10 -R $REF -I $OUT/${i}_sorted_rmdup_sorted.bam -o $OUT/${i}_realignertargetcreator.intervals
    # IndelRealigner
    java -jar /home/tmp/Software/GATK_3.4/GenomeAnalysisTK.jar -T IndelRealigner -R $REF -targetIntervals $OUT/${i}_realignertargetcreator.intervals -I $OUT/${i}_sorted_rmdup_sorted.bam -o $OUT/${i}_sorted_rmdup_sorted_indelrealigner.bam
    samtools flagstat $OUT/${i}_sorted_rmdup_sorted_indelrealigner.bam > $OUT/${i}_sorted_rmdup_sorted_indelrealigner.stats

done

```

##  Base Quality Score Recalibration

This step is based on a previously known set of SNPs which are highly reliable, and can be used as guideline for quality score recalibration. As we don't have this set of SNPs, we are considering skipping this step, even though the GATK guidelines suggest doing it anyways. Talking to Murphy will clarify if this step is necessary or not.

```

```

## Mapping Loop

```
# MACROGEN
screen -S maploop.log
script maploop.log
./MACROGEN-mapping-noLL112.sh

# PGENOMA
screen -S pgenmaploop.log
script pgenmaploop.log
./PGENOMA-mapping.sh

# AMERICALYNX
screen -S americamaploop.log
script americamaploop.log
./AMERICALYNX-mapping.sh

# PGENOMA2
screen -S pgenma2ploop.log
script pgenma2ploop.log
./PGENOMA-mapping-part2.sh

# BOBCAT1
screen -S BOBCAT1map.log
script BOBCAT1map.log
./BOBCAT1-mapping.sh

# CANDILES
screen -S CANDILESmap.log
script CANDILESmap.log
./CANDILES-mapping.sh


```

# Depth Calculations

This step, developed by Maria, will generate a table with depth calculations and overall stats.

```
## Samtools: Maria version
# If I run it again add the last sed after the loop so that is comparable with the previous stats!!!
SAMPLELIST=($(ls *_indelrealigner.bam | uniq ))
rm global_coverage.tsv
echo -e "sample_name\tcoverage_based_samtools\tstdev_based_samtools" > global_coverage.tsv
for sample in "${SAMPLELIST[@]}"
do
echo $sample
DEPTH=$(samtools depth $sample | awk '{sum+=$3; sumsq+=$3*$3} END { print sum/LENGTHOFCATGENOME; print sqrt(sumsq/LENGTHOFCATGENOME - (sum/LENGTHOFCATGENOME)**2)}') ## or flag -a with /NR instead of /LENGTHOFCATGENOME

samtools depth -a CatRef_bams/c_ll_vl_0112_cat_ref_sorted_rg_rmdup_sorted_indelrealigner.bam | awk '{sum+=$3; sumsq+=$3*$3} END { print sum/NR; print sqrt(sumsq/NR - (sum/NR)**2)}'
# 27.8014
# 39.8728

samtools depth CatRef_bams/c_ll_vl_0112_cat_ref_sorted_rg_rmdup_sorted_indelrealigner.bam | awk '{sum+=$3; sumsq+=$3*$3} END { print sum/2563897203; print sqrt(sumsq/2563897203 - (sum/2563897203)**2)}'

# 27.2747
# 39.6748

paste \
<(echo $sample ) \
<(echo $DEPTH ) |\
sed 's/ /\t/g'| sed 's/\t\+/\t/g' >>  global_coverage.tsv
done
# I modify the name so that it has a common field with the previous report:
cat global_coverage.tsv | sed 's/_recal_round-1.bam//g' | sed 's/\t/,/g' > coverage_samtools
rm global_coverage.tsv

```
```
## To calculate number of bases with at least X depth
samtools mpileup mapping_result_sorted.bam | awk -v X="${MIN_COVERAGE_DEPTH}" '$4>=X' | wc -l

```
```
## Calculate percentage of genome covered at more than 1 base depth

#Determine number of bases at 0 read depth
zero=$(bedtools genomecov \
  -ibam CatRef_bams/c_ll_vl_0112_cat_ref_sorted_rg_rmdup_sorted_indelrealigner.bam \
  -g /home/GRUPOS/grupolince/reference_genomes/felis_catus_genome/Felis_catus.Felis_catus_9.0.dna.toplevel.fa \
  -bga | awk '$4==0 {bpCountZero+=($3-$2)} {print bpCountZero}' | tail -1)

#Determine number of bases at >0 read depth, i.e., non-zero bases
nonzero=$(bedtools genomecov -ibam c_ll_vl_0112_cat_ref_sorted_rg_rmdup_sorted_indelrealigner.bam \
  -g /home/GRUPOS/grupolince/reference_genomes/felis_catus_genome/Felis_catus.Felis_catus_9.0.dna.toplevel.fa \
  -bga | awk '$4>0 {bpCountNonZero+=($3-$2)} {print bpCountNonZero}' | tail -1)

#Calculate the percent of the reference genome coverd by >0 read depth bases
#Round up to 6 decimal places
percent=$(bc <<< "scale=6; ($nonzero / ($zero + $nonzero))*100")

echo $percent


length=2563897203
percent=$(bc <<< "scale=6; ($length - $zero)/($length)*100")
echo $percent
# 95.104700


```


# Number of Mismatches (NM) Calculations

This step, developed by Daniel, is run in order to evaluate how much "dirty" reads there are in your mapping, and how applying a filter for NM-per-read would affect the total amount of reads/coverage.

```
screen -S nm_calculations.log
script nm_calculations.log

cd ~/CatRef_bams
declare SAMPLES=$(ls *_indelrealigner.bam | cut -d'_' -f1-4 | sort | uniq)
for i in ${SAMPLES[@]}
  do
  echo "${i}"
  samtools view "${i}"_cat_ref_sorted_rg_rmdup_sorted_indelrealigner.bam | shuf -n 1000000 | grep -o '\bNM:i:\w*' | cut -d':' -f3 > ~/nm_distributions/"${i}"_NM_distr.txt
done

```

After copying these TXT files to local computer, I will process them with R.
I also generated a file with the names of all of the individuals I mapped and the "project" they were sequenced in:
samples_projectsnames.txt

```{r}

library(readr)
library(dplyr)
library(ggplot2)

#First draw the NM distribution for each individual:
sample_files <- list.files("/Users/enricobazzicalupo/Documents/PlanNacional/nm_distributions/", pattern="*_distr.txt")
for (file in sample_files) {
  edit_distance_distribution <- read_tsv(paste0("/Users/enricobazzicalupo/Documents/PlanNacional/nm_distributions/",file),col_names=c("NM"))
  edit_distance_distribution
  sample <- paste(strsplit(file,"_")[[1]][c(1:4)], collapse = '_')
  print(sample)
  plot_data <- edit_distance_distribution %>% group_by(NM) %>% tally()
  plot_data
  #edit_distance_distribution$pop <- as.factor(edit_distance_distribution$pop)
  #plot_data <- edit_distance_distribution %>% filter(edit_distance_distribution$pop == !!pop) #the two !! allow R to evaluate the text and distinguish looping variables from col_names
  #plot_data
  NM_distr_ggplot <- ggplot(data=plot_data, aes(NM,n)) +
  #geom_histogram(aes(NM),binwidth=1) +
  geom_col(width = 1) +
  ggtitle(paste0("NM distribution for ",sample)) +
  ylab("count") +
  theme_bw() +
  theme(text=element_text(size=12,face="bold"),
        rect=element_rect(size=1),
        axis.line=element_line(colour="black"),
        axis.title=element_text(size=16),
        #axis.text.x=element_text(angle=45, hjust=1, size=24,colour="black"),
        #axis.text.y=element_text(size=24,colour="black",margin=margin(t=0.5,unit="cm")),
        #axis.title.y=element_text(size=30,margin=margin(r=0.5,unit="cm")),
        panel.background=element_blank(),
        panel.border=element_rect(colour="black"),
        #panel.grid=element_blank(),
        #panel.grid.major=element_line(colour="grey", linetype="dashed", size=0.4),
        plot.margin=unit(c(0.5,1,0.5,0.5),"cm"),
        #plot.title=element_text(size=36, face="bold", margin=margin(b=0.5, unit="cm")),
        legend.background=element_rect(linetype="solid", colour="black", size=.5),
        #legend.justification=c(0,0),
        legend.key=element_rect(colour="white"),
        #legend.key.size=unit(1.3,"cm"),
        legend.position=c(0.92,0.86),
        legend.title=element_blank()
  )
  NM_distr_ggplot
  ggsave(paste0(sample,"_NM_distribution.genes.pdf"), width=30, height=20, units="cm", device="pdf", path="/Users/enricobazzicalupo/Documents/PlanNacional/nm_distributions/")
}
rm(edit_distance_distribution)
#Draw proportion of reads at different NMs:
sample_files <- list.files("/Users/enricobazzicalupo/Documents/PlanNacional/nm_distributions/", pattern="*_distr.txt")
sample_projects <- read_tsv("/Users/enricobazzicalupo/Documents/PlanNacional/samples_projectsnames.txt", col_names = c("ind", "project"))
all_together <- data_frame()
for (file in sample_files) {
  edit_distance_distribution <- read_tsv(paste0("/Users/enricobazzicalupo/Documents/PlanNacional/nm_distributions/",file),col_names=c("NM"))
  edit_distance_distribution
  sample <- paste(strsplit(file,"_")[[1]][c(1:4)], collapse = '_')
  print(sample)
  plot_data <- edit_distance_distribution %>% group_by(NM) %>% tally()
  plot_data
  reads_totales <- sum(plot_data$n)
  plot_bis <- mutate(plot_data,prop=as.numeric(100*n/reads_totales))
  plot_bis$cum_prop <- cumsum(plot_bis$prop)
  plot_bis$sample <- c(sample)
  #plot_bis$dataset <- ifelse(plot_bis$sample=="0007" | plot_bis$sample=="0153" | plot_bis$sample=="0173" | plot_bis$sample=="0443", "GP",ifelse())
  #plot_bis$project <- c(unlist(sample_projects[which(sample_projects[,1]==sample),2], use.names = F))
  plot_bis <- left_join(plot_bis,sample_projects, by = c("sample" = "ind"))
  plot_bis$rlength <- ifelse(plot_bis$project=="MURPHY", 125, ifelse(plot_bis$project=="PGENOMA", 100, 151))
  plot_bis$divpercent <- plot_bis$NM / plot_bis$rlength *100
  #plot_bis
  #all_together <- rbind(all_together,plot_bis)
  all_together <- rbind(all_together,plot_bis[c(1:21),])

  all_together
}
NM_freq_ggplot <- ggplot(data=all_together, aes(divpercent,prop,colour=project)) +
#geom_histogram(aes(NM),binwidth=1) +
geom_point() +
ggtitle("Proportion of reads at different NM") +
ylab("percentage") +
theme_bw() +
theme(text=element_text(size=12,face="bold"),
      rect=element_rect(size=1),
      axis.line=element_line(colour="black"),
      axis.title=element_text(size=16),
      #axis.text.x=element_text(angle=45, hjust=1, size=24,colour="black"),
      #axis.text.y=element_text(size=24,colour="black",margin=margin(t=0.5,unit="cm")),
      #axis.title.y=element_text(size=30,margin=margin(r=0.5,unit="cm")),
      panel.background=element_blank(),
      panel.border=element_rect(colour="black"),
      #panel.grid=element_blank(),
      #panel.grid.major=element_line(colour="grey", linetype="dashed", size=0.4),
      plot.margin=unit(c(0.5,1,0.5,0.5),"cm"),
      #plot.title=element_text(size=36, face="bold", margin=margin(b=0.5, unit="cm")),
      legend.background=element_rect(linetype="solid", colour="black", size=.5),
      #legend.justification=c(0,0),
      legend.key=element_rect(colour="white"),
      #legend.key.size=unit(1.3,"cm"),
      legend.position=c(0.92,0.86),
      legend.title=element_blank()
)
NM_freq_ggplot
ggsave("proportion_of_reads_at_different_NM.genes.pdf", width=15, height=10, units="cm", device="pdf", path="/Users/enricobazzicalupo/Documents/PlanNacional/nm_distributions/")


```
Finding out read length to see if the difference in NM distribution might come from there. As there are read length differences I need to normalize the NM calculations with respect to read length.

```
## MACROGEN
zcat /backup/grupolince/raw_data/MACROGEN/MACROGEN_trimmed/LC1_R2_trimmed.fastq.gz | head -100 | awk 'NR%4 == 2 {lengths[length($0)]++} END {for (l in lengths) {print l, lengths[l]}}'
# 151bp

## PGENOMA
cd /home/ebazzicalupo/fastqs/Enrico_move
declare FASTQS=$(ls *.fastq.gz)
for i in $FASTQS
  do
    zcat $i | head -100 | awk 'NR%4 == 2 {lengths[length($0)]++} END {for (l in lengths) {print l, lengths[l]}}'
done
# 100 bp

## MURPHY
cd /GRUPOS/grupolince/PN2017/LCA_3/
declare FASTQS=$(ls *.fastq.gz)
for i in $FASTQS
  do
    zcat $i | head -100 | awk 'NR%4 == 2 {lengths[length($0)]++} END {for (l in lengths) {print l, lengths[l]}}'
done
# 125 bp



```
