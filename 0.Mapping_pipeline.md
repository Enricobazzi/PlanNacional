---
title: "0.Mapping_pipeline"
author: "Enrico"
date: "30 January 2019"
output: html_document
---

In order to calculate demographic models with machine learning methods (step carried by other group = Oscar Lao), high coverage sequencing data for at least 2 individuals per species will be mapped to latest version of the cat reference genome (version 9.0 -> downloaded from ensembl and copied to /home/GRUPOS/grupolince/reference_genomes/felis_catus_genome folder).

In this markdown I will go through the steps necessary for the mapping pipeline, to test each step with only one individual from the MACROGEN sequencing, LL112 (c_ll_vl_0112). The executable files generated afterwards (for mapping the rest of the samples), go through the same steps.

```

# BEFORE STARTING
# Reference Genome needs indexing
samtools faidx /home/GRUPOS/grupolince/reference_genomes/felis_catus_genome/Felis_catus.Felis_catus_9.0.dna.toplevel.fa

java -jar /opt/picard-tools/picard.jar CreateSequenceDictionary R= /home/GRUPOS/grupolince/reference_genomes/felis_catus_genome/Felis_catus.Felis_catus_9.0.dna.toplevel.fa O= /home/GRUPOS/grupolince/reference_genomes/felis_catus_genome/Felis_catus.Felis_catus_9.0.dna.toplevel.dict

bwa index /home/GRUPOS/grupolince/reference_genomes/felis_catus_genome/Felis_catus.Felis_catus_9.0.dna.toplevel.fa

```

Since the samples were already quality checked and trimmed for previous projects, these steps were skipped and I proceeded to map directly.

# Path Definition

Define the paths to different locations

```

MacroGenARRAY=($(ls /backup/grupolince/raw_data/MACROGEN/MACROGEN_trimmed/*.fastq.gz | rev | cut -d'/' -f 1 | rev | cut -d '_' -f1 | uniq)) # List of all Lynx lynx sample codes in MACROGEN project

REF=/home/GRUPOS/grupolince/reference_genomes/felis_catus_genome/Felis_catus.Felis_catus_9.0.dna.toplevel.fa # path to cat reference genome

THREADS=10 # No. of computer cores used by bwa and samtools. 20 = OK, >20 = ask people first!

OUT=/home/ebazzicalupo/CatRef_bams # path to output files

MacroGenPATH=/backup/grupolince/raw_data/MACROGEN/MACROGEN_trimmed/ # path to macrogen fastq files


# BARCODE MACROGEN:
declare -A BARCODEID=(["LC1"]="c_lc_zz_0001" ["LL112"]="c_ll_vl_0112" ["LL146"]="c_ll_ya_0146" ["LL212"]="c_ll_cr_0212" ["LL90"]="c_ll_ki_0090" ["LR1"]="c_lr_zz_0001")

```

# 	Mapping BWA-MEM

```

# Trial with only second (1) array element -> LL112

screen -S CatRef_Mapping.log
script CatRef_Mapping.log

for i in ${MacroGenARRAY[1]}
  do
    echo " - mapping ${i} -"
    bwa mem $REF $MacroGenPATH/${i}_R1_trimmed.fastq.gz $MacroGenPATH/${i}_R2_trimmed.fastq.gz -t $THREADS | samtools view -hbS -@ $THREADS - -o $OUT/${i}.cat_ref.bam
    echo " - sorting ${i} -"
    samtools sort -@ $THREADS $OUT/${i}.cat_ref.bam -o $OUT/${i}.cat_ref.sorted.bam && rm $OUT/${i}.cat_ref.bam
    echo " - done -"
done

```

## 	Add Read Groups and Change BAM names

Some of the merging part of this script was not actually used. As Maria was merging different sequencing projects at the same time she needed to include an "if statement" (see below) to see if merging was actually necessary. As I mapped different projects at different times, in the end I knew if merging was necessary or not for each, so I removed the "if statement" and included the merging step only when necessary.

```

# Trial with only second (1) array element -> LL112

# Read Groups

screen -S ReadGroups.log
script ReadGroups.log

for i in ${MacroGenARRAY[1]}
  do
    run=($(echo $i | cut -d"_" -f 1))  #Sacar el run de i
    echo $run
    java -jar /opt/picard-tools/picard.jar AddOrReplaceReadGroups I=$OUT/${i}.cat_ref.sorted.bam O=$OUT/${BARCODEID["${i}"]}_${i}.cat_ref.sorted.rg.bam RGID=${i} RGLB=${BARCODEID["${i}"]}_lib RGPL=Illumina RGPU=${run} RGSM=${BARCODEID["${i}"]} VALIDATION_STRINGENCY=SILENT && rm $OUT/${i}.cat_ref.sorted.bam
done

# Merge BAMs / Change BAM names

SAMPLESLIST=($(echo ${BARCODEID[@]} | tr ' ' '\n' | sort | uniq | grep c_ll_vl_0112))
for sample in "${SAMPLESLIST[@]}"
  do
    echo "${sample}"
    ls $OUT/"${sample}"_*.cat_ref.sorted.rg.bam  > $OUT/"${sample}".bam.list
    echo;echo
    echo `wc -l $OUT/"${sample}".bam.list`;
    lines=`wc -l $OUT/"${sample}".bam.list | cut -f 1 -d " " `
    if [ "$lines" -eq "1" ];
      then echo "ONE";
      cp `cat $OUT/"${sample}".bam.list`  $OUT/"${sample}"_sorted.bam;
    elif [ "$lines" -gt "1" ];
      then echo "more than ONE";
      samtools merge  -@ $THREADS -r $OUT/"${sample}".bam `cat $OUT/"${sample}".bam.list`;
      samtools sort  -@ $THREADS $OUT/"${sample}".bam -o $OUT/"${sample}"_sorted.bam && rm $OUT/"${sample}".bam;
    fi
    samtools flagstat $OUT/"${sample}"_sorted.bam >  $OUT/"${sample}".stats;
done

```

## 	Remove / Mark Duplicates

```

screen -S MDUP.log
script MDUP.log

ARRAY_MERGED_BAM_SAMPLE_NAME=($(ls $OUT/*_sorted.bam | rev | cut -d'/' -f1 | rev | cut -d'_' -f1-4 | uniq))
for i in ${ARRAY_MERGED_BAM_SAMPLE_NAME[@]}
  do
  echo "${i}"
  java -jar /opt/picard-tools/picard.jar MarkDuplicates METRICS_FILE=${i}_rmdup.txt I=$OUT/${i}_sorted.bam O=$OUT/${i}_sorted_rmdup.bam MAX_FILE_HANDLES_FOR_READ_ENDS_MAP=800
  rm $OUT/${i}_sorted.bam
  samtools sort $OUT/${i}_sorted_rmdup.bam -@ 10 -o $OUT/${i}_sorted_rmdup_sorted.bam
  rm $OUT/${i}_sorted_rmdup.bam
  samtools index $OUT/${i}_sorted_rmdup_sorted.bam
  samtools flagstat $OUT/${i}_sorted_rmdup_sorted.bam > $OUT/${i}_sorted_rmdup_sorted.stats
done

```

##  Realign

```

screen -S realign.log
script realign.log

ARRAY_MERGED_BAM_SAMPLE_NAME=($(ls $OUT/*_sorted.bam | rev | cut -d'/' -f1 | rev | cut -d'_' -f1-4 | uniq))

## Not INCLUDING -fixMisencodedQuals flag because MACROGEN are Illumina 1.9 (quality code doesn't need fixing)

for i in ${ARRAY_MERGED_BAM_SAMPLE_NAME[@]}
  do
    echo "${i}"
    samtools index $OUT/${i}_sorted_rmdup_sorted.bam
    # RealignerTargetCreator
    java -jar /home/tmp/Software/GATK_3.4/GenomeAnalysisTK.jar -T RealignerTargetCreator -nt 10 -R $REF -I $OUT/${i}_sorted_rmdup_sorted.bam -o $OUT/${i}_realignertargetcreator.intervals
    # IndelRealigner
    java -jar /home/tmp/Software/GATK_3.4/GenomeAnalysisTK.jar -T IndelRealigner -R $REF -targetIntervals $OUT/${i}_realignertargetcreator.intervals -I $OUT/${i}_sorted_rmdup_sorted.bam -o $OUT/${i}_sorted_rmdup_sorted_indelrealigner.bam
    samtools flagstat $OUT/${i}_sorted_rmdup_sorted_indelrealigner.bam > $OUT/${i}_sorted_rmdup_sorted_indelrealigner.stats

done

```

##  Base Quality Score Recalibration

This step is based on a previously known set of SNPs which are highly reliable, and can be used as guideline for quality score recalibration. As we don't have this set of SNPs, we are considering skipping this step, even though the GATK guidelines suggest doing it anyways. Talking to Murphy will clarify if this step is necessary or not.

```

```

# Mapping Loop

```
# MACROGEN
screen -S maploop.log
script maploop.log
./MACROGEN-mapping-noLL112.sh

# PGENOMA
screen -S pgenmaploop.log
script pgenmaploop.log
./PGENOMA-mapping.sh

# AMERICALYNX
screen -S americamaploop.log
script americamaploop.log
./AMERICALYNX-mapping.sh

# PGENOMA2
screen -S pgenma2ploop.log
script pgenma2ploop.log
./PGENOMA-mapping-part2.sh


```

# Depth Calculations

This step, developed by Maria, will generate a table with depth calculations and overall stats.

```
# Samtools: The one I am doing
# If I run it again add the last sed after the loop so that is comparable with the previous stats!!!
SAMPLELIST=($(ls *_indelrealigner.bam | uniq ))
rm global_coverage.tsv
echo -e "sample_name\tcoverage_based_samtools\tstdev_based_samtools" > global_coverage.tsv
for sample in "${SAMPLELIST[@]}"
do
echo $sample
DEPTH=$(samtools depth $sample | awk '{sum+=$3; sumsq+=$3*$3} END { print sum/LENGTHOFCATGENOME; print sqrt(sumsq/LENGTHOFCATGENOME - (sum/LENGTHOFCATGENOME)**2)}') ## or flag -a with /NR instead of /LENGTHOFCATGENOME

samtools depth -a CatRef_bams/c_ll_vl_0112_cat_ref_sorted_rg_rmdup_sorted_indelrealigner.bam | awk '{sum+=$3; sumsq+=$3*$3} END { print sum/NR; print sqrt(sumsq/NR - (sum/NR)**2)}'
# 27.8014
# 39.8728

samtools depth CatRef_bams/c_ll_vl_0112_cat_ref_sorted_rg_rmdup_sorted_indelrealigner.bam | awk '{sum+=$3; sumsq+=$3*$3} END { print sum/2563897203; print sqrt(sumsq/2563897203 - (sum/2563897203)**2)}'





paste \
<(echo $sample ) \
<(echo $DEPTH ) |\
sed 's/ /\t/g'| sed 's/\t\+/\t/g' >>  global_coverage.tsv
done
# I modify the name so that it has a common field with the previous report:
cat global_coverage.tsv | sed 's/_recal_round-1.bam//g' | sed 's/\t/,/g' > coverage_samtools
rm global_coverage.tsv

## To calculate number of bases with at least X depth
samtools mpileup mapping_result_sorted.bam | awk -v X="${MIN_COVERAGE_DEPTH}" '$4>=X' | wc -l


```


# Number of Mismatches (NM) Calculations

This step, developed by Daniel, is run in order to evaluate how much "dirty" reads there are in your mapping, and how applying a filter for NM-per-read would affect the total amount of reads/coverage.

```
screen -S nm_calculations.log
script nm_calculations.log

cd ~/CatRef_bams
declare SAMPLES=$(ls *_indelrealigner.bam | cut -d'_' -f1-4 | sort | uniq)
for i in ${SAMPLES[@]}
  do
  echo "${i}"
  samtools view "${i}"_cat_ref_sorted_rg_rmdup_sorted_indelrealigner.bam | shuf -n 1000000 | grep -o '\bNM:i:\w*' | cut -d':' -f3 > ~/nm_distributions/"${i}"_NM_distr.txt
done


```
